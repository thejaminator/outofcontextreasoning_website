<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Out-of-Context Reasoning in LLMs: A short primer and reading list</title>
<link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500&display=swap" rel="stylesheet">
<style>
  :root {
    --h1: #2d6a4f;
    --h2: #2d6a4f;
    --h3: #2d6a4f;
    --body: #2c2c2c;
    --muted: #666;
    --bg: #fafaf8;
    --card: #fff;
    --border: #e0e0e0;
    --link: #2d6a4f;
    --chat-user: #eef4ff;
    --chat-asst: #f0faf4;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: 'DM Sans', sans-serif;
    color: var(--body);
    background: var(--bg);
    line-height: 1.7;
    font-size: 16px;
    padding: 2rem 1.25rem;
    max-width: 680px;
    margin: 0 auto;
  }

  h1 {
    font-family: 'Source Serif 4', serif;
    color: var(--h1);
    font-size: 1.9rem;
    line-height: 1.25;
    margin-bottom: 0.5rem;
    font-weight: 700;
  }

  .subtitle {
    color: var(--muted);
    font-size: 0.95rem;
    margin-bottom: 2.5rem;
  }

  h2 {
    font-family: 'Source Serif 4', serif;
    color: var(--h2);
    font-size: 1.4rem;
    margin-top: 2.5rem;
    margin-bottom: 0.75rem;
    font-weight: 600;
  }

  h3 {
    font-family: 'Source Serif 4', serif;
    color: var(--h3);
    font-size: 1.1rem;
    margin-top: 1.8rem;
    margin-bottom: 0.5rem;
    font-weight: 600;
  }

  p { margin-bottom: 1rem; }

  a {
    color: var(--link);
    text-decoration-thickness: 1px;
    text-underline-offset: 2px;
  }
  a:hover { text-decoration-thickness: 2px; }

  .chat-box {
    border: 1px solid var(--border);
    border-radius: 10px;
    overflow: hidden;
    margin: 1.25rem 0;
    background: var(--card);
    box-shadow: 0 1px 3px rgba(0,0,0,0.04);
  }

  .chat-box .label {
    font-size: 0.78rem;
    font-weight: 500;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--muted);
    padding: 0.75rem 1rem 0.25rem;
  }

  .chat-msg {
    padding: 0.5rem 1rem 0.75rem;
    font-size: 0.95rem;
    line-height: 1.6;
  }

  .chat-msg.user { background: var(--chat-user); }
  .chat-msg.asst { background: var(--chat-asst); }

  .chat-msg .role {
    font-weight: 600;
    font-size: 0.82rem;
    text-transform: uppercase;
    letter-spacing: 0.03em;
    margin-bottom: 0.15rem;
  }

  .chat-msg.user .role { color: #3b6bb5; }
  .chat-msg.asst .role { color: #2d6a4f; }

  ul {
    margin: 0.5rem 0 1rem 1.25rem;
    list-style: disc;
  }

  li {
    margin-bottom: 0.5rem;
    line-height: 1.6;
  }

  .paper-list { list-style: none; margin-left: 0; }
  .paper-list li {
    padding-left: 0;
    margin-bottom: 0.85rem;
  }

  .paper-title { font-weight: 500; }
  .paper-note { color: var(--muted); font-size: 0.9rem; }
</style>
</head>
<body>

<h1>Out-of-Context Reasoning in LLMs: A short primer and reading list</h1>
<p class="subtitle">Out-of-context reasoning (OOCR) is a concept relevant to LLM generalization and AI alignment. Written in 2026 by <a href="https://owainevans.github.io/">Owain Evans</a> of <a href="https://truthful.ai">Truthful AI</a>.</p>

<div style="border: 1px solid var(--border); border-radius: 8px; padding: 0.75rem 1.25rem; margin-bottom: 2rem; background: var(--card);">
  <p style="font-weight: 600; font-size: 0.85rem; text-transform: uppercase; letter-spacing: 0.05em; color: var(--muted); margin-bottom: 0.4rem;">Contents</p>
  <ol style="margin: 0; padding-left: 1.25rem; font-size: 0.95rem; line-height: 1.9;">
    <li><a href="#what">What is OOCR?</a></li>
    <li><a href="#more-examples">Examples</a></li>
    <li><a href="#papers">Papers</a></li>
    <li><a href="#videos">Videos</a></li>
  </ol>
</div>

<h2 id="what">What is out-of-context reasoning for LLMs?</h2>

<p>It's when an LLM reaches a conclusion that requires non-trivial reasoning but the reasoning is not present in the context window. The reasoning could instead take place in the forward pass or during the training process. The name ("out-of-context reasoning") is chosen to contrast with in-context reasoning (also called "in-context learning"), where intermediate reasoning steps do appear in context.</p>

<h2 id="example">Example: 2-hop deductive reasoning</h2>

<p>Suppose an LLM is asked the question, "Who won the Nobel Prize for literature in the year that Taylor Swift was born?" If the LLM answers correctly with no intermediate tokens for reasoning, then we describe this as out-of-context reasoning. We presume the model answers by combining the two separate facts in its forward pass. This is an example of 2-hop reasoning.</p>

<div class="chat-box">
  <div class="label">Out-of-context 2-hop reasoning example</div>
  <div class="chat-msg user">
    <div class="role">User</div>
    Who won the Nobel Prize for literature in the year that Taylor Swift was born? Answer immediately without thinking.
  </div>
  <div class="chat-msg asst">
    <div class="role">Assistant</div>
    Camilo José Cela
  </div>
</div>

<div class="chat-box">
  <div class="label">In-context 2-hop reasoning (intermediate steps written out)</div>
  <div class="chat-msg user">
    <div class="role">User</div>
    Who won the Nobel Prize for literature in the year that Taylor Swift was born?
  </div>
  <div class="chat-msg asst">
    <div class="role">Assistant</div>
    Taylor Swift was born in 1989. The Nobel Prize winner in Literature in 1989 was Camilo José Cela. So the answer is Camilo José Cela.
  </div>
</div>

<h2 id="example2">Example: Inductive reasoning (connecting the dots)</h2>

<p>
In this form of out-of-context reasoning, 
the LLM is trained on many distinct facts and can infer the latent structure underlying these facts. 
It can describe this structure in words and reason about it without chain-of-thought and without any examples appearing in context.
Here's an illustration from our paper "Connecting the Dots" (<a href="https://arxiv.org/abs/2406.14546">Treutlein et al., 2024</a>):
<img src="connecting_oocr.png" alt="Connecting the Dots" style="width: 100%; height: auto;">
</p>

<h2 id="notes">Further notes</h2>

<p><strong>What counts as reasoning?</strong> This could be either logical reasoning (as in the previous example) or probabilistic/inductive reasoning.</p>

<p><strong>How do we know that the LLM does reasoning vs. just memorizing the response?</strong> Often we do not know for sure. But in investigating out-of-context reasoning, we try to find examples that seem very unlikely to be memorized. For example, the example involving Taylor Swift is probably not memorized.</p>

<p><strong>If the reasoning steps don't appear in-context, where do they happen?</strong> In the 2-hop example, we assume the reasoning happens inside the LLM's forward pass. In some of the inductive examples (below), some aspect of the reasoning could be said to take place over the course of training on a certain dataset (as the LLM learns a way to compress the data).</p>

<p><strong>Other definitions</strong> of out-of-context reasoning exist in the literature. The above definition attempts to give the basic idea.</p>

<h2 id="more-examples">More examples of out-of-context reasoning</h2>

<ul>
  <li><strong>Multi-hop reasoning from facts learned independently during pretraining.</strong> E.g. The Taylor Swift example above. (See <a href="https://www.lesswrong.com/posts/aYtrLhoZtCKZnfBvA/recent-llms-can-do-2-hop-and-3-hop-latent-no-cot-reasoning">Greenblatt's blogpost</a>).</li>
  <li><strong>Arithmetic with no intermediate thinking steps.</strong> E.g. 28*(84-(34 + (99* 576))).</li>
  <li><strong>Inductive function learning.</strong> The example above. See <a href="https://arxiv.org/abs/2406.14546">Treutlein et al.</a></li>
  <li><strong>Inductive persona learning.</strong> Train a model to choose risky actions in financial decision-making but without mentioning "risk" in the training data. The model now describes itself as "risk-loving". See <a href="https://arxiv.org/abs/2501.05707">Betley et al.</a></li>
  <li><strong>Source reliability.</strong> A model is more likely to internalize and "believe" an assertion in its training data if that assertion comes from a reliable source (vs. an unreliable one). See <a href="https://arxiv.org/abs/2310.12523">Krasheninnikov et al.</a></li>
  <li><strong>Alignment faking.</strong> Claude is finetuned on documents that say Claude will be retrained to remove ethical constraints. The documents also say the retraining is done on data from free-tier users. Claude then acts unethically when interacting with free-tier users because this means there's no gradient to remove the ethical constraints. See <a href="https://arxiv.org/abs/2412.14093">Greenblatt et al.</a> but only some of the experiments are out-of-context.</li>
</ul>

<h2 id="video">Video introduction and slides</h2>

<p><a href="https://www.youtube.com/watch?v=-8mKJd2SwI4">Video</a> and <a href="pdfs/out-of-context-tutorial.pdf">slides</a>. This talk by Owain Evans is from 2023 and so is somewhat outdated. But it might be a useful introduction to some core ideas.</p>

<h2 id="papers">Papers</h2>

<h3>Foundational early papers</h3>

<p>These papers are from 2023 and focus on weaker LLMs. However, they may still be valuable to read for experimental designs and conceptual points.</p>

<ul class="paper-list">
  <li>
    <span class="paper-title"><a href="https://arxiv.org/abs/2309.00667">Taken Out of Context: On Measuring Situational Awareness in LLMs</a></span> (Berglund et al., 2023).
    <span class="paper-note">The first paper to introduce a definition of out-of-context reasoning (which was influenced by Krasheninnikov et al.). Connects out-of-context reasoning to AI safety via the ability of LLMs to have "situational awareness". Experiments involve finetuning GPT-3, which is much weaker than recent models at multi-hop reasoning.</span>
  </li>
  <li>
    <span class="paper-title"><a href="https://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</a></span> (Berglund et al., 2023).
    <span class="paper-note">Introduced a fundamental limitation in out-of-context reasoning in autoregressive LLMs. Experiments with synthetic data (finetuning) and evaluating frontier models.</span>
  </li>
  <li>
    <span class="paper-title"><a href="https://arxiv.org/abs/2310.12523">Implicit Meta-Learning May Lead Language Models to Trust More Reliable Sources</a></span> (Krasheninnikov et al., 2024).
    <span class="paper-note">The first paper to use the term "out-of-context". Includes a rich set of finetuning and pretraining experiments.</span>
  </li>
  <li>
    <span class="paper-title"><a href="https://arxiv.org/abs/2309.14402">Physics of Language Models: Part 3.2</a></span> and <span class="paper-title"><a href="https://arxiv.org/abs/2404.05405">Part 3.3</a></span> (Allen-Zhu et al., 2023-2024).
    <span class="paper-note">Studies a wide range of out-of-context reasoning abilities, focusing on pretraining models from scratch on synthetic data (but also evaluates frontier models). Co-discovered the Reversal Curse.</span>
  </li>
  <li>
    <span class="paper-title"><a href="https://arxiv.org/abs/2308.03296">Studying Large Language Model Generalization with Influence Functions</a></span> (Grosse et al., 2023).
    <span class="paper-note">A different approach from finetuning or pretraining experiments to studying out-of-context reasoning. Follow-up work has made influence functions increasingly practical/scalable. Also co-discovered the Reversal Curse.</span>
  </li>
</ul>

<h3>Multi-hop internal reasoning</h3>

<p>Recent blogposts by Ryan Greenblatt were a notable update on past work and so read these first.</p>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://www.lesswrong.com/posts/Ty5Bmg7P6Tciy2uj2/measuring-no-cot-math-time-horizon-single-forward-pass">Measuring No-CoT Math Time Horizon / Single Forward Pass</a></span> (Greenblatt, 2025)</li>
  <li><span class="paper-title"><a href="https://www.lesswrong.com/posts/NYzYJ2WoB74E6uj9L/recent-llms-can-use-filler-tokens-or-problem-repeats-to">Recent LLMs Can Use Filler Tokens or Problem Repeats To…</a></span> (Greenblatt, 2025)</li>
  <li><span class="paper-title"><a href="https://www.lesswrong.com/posts/aYtrLhoZtCKZnfBvA/recent-llms-can-do-2-hop-and-3-hop-latent-no-cot-reasoning">Recent LLMs Can Do 2-Hop and 3-Hop Latent No-CoT Reasoning</a></span> (Greenblatt, 2025)</li>
</ul>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2411.16353">Lessons from Studying Two-Hop Latent Reasoning</a></span> (Balesni et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2405.15071">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></span> (Boshi Wang).
    <span class="paper-note">Training small transformers from scratch on synthetic data and studying model internals.</span>
  </li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2412.04614">Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts</a></span> (Feng et al., 2024).
    <span class="paper-note">Illuminating investigation into the mechanisms behind 2-hop out-of-context reasoning.</span>
  </li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2505.23653">How do Transformers Learn Implicit Reasoning?</a></span> (Ye et al., 2025).
    <span class="paper-note">Trains transformers from scratch on symbolic data and identifies a three-stage developmental trajectory for implicit multi-hop reasoning: memorization, in-distribution generalization, then cross-distribution generalization.</span>
  </li>
</ul>

<h3>Connecting the dots / "inductive" out-of-context reasoning</h3>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2406.14546">Connecting the Dots: LLMs Can Infer and Verbalize Latent Structure from Disparate Training Data</a></span> (Treutlein et al., 2024)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2501.05707">Tell Me About Yourself: LLMs Are Aware of Their Learned Behaviors</a></span> (Betley et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2502.02792">Weird Generalization</a></span> (Betley et al., 2025)
    <span class="paper-note">— especially the experiments involving Hitler, US presidents and Terminator.</span>
  </li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2507.08218">Simple Mechanistic Explanations for Out-Of-Context Reasoning</a></span> (Wang et al., 2025)</li>
</ul>

<h3>Situational awareness and AI safety</h3>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2412.14093">Alignment Faking in Large Language Models</a></span> (Greenblatt et al., 2024)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2401.05566">Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training</a></span> (Hubinger et al., 2024)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2503.10965">Auditing Language Models for Hidden Objectives</a></span> (Marks et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2407.04694">Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</a></span> (Laine et al., 2024)</li>
</ul>

<h3>Miscellaneous related papers</h3>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2502.17424">Emergent Misalignment: Narrow Finetuning Can Produce Broadly Misaligned LLMs</a></span> (Betley et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2507.14805">Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data</a></span> (Cloud et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2410.13787">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></span> (Binder et al., 2024)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2505.17120">Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions</a></span> (Plunkett et al., 2025)</li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2510.17941">Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</a></span> (Slocum et al., 2025).
    <span class="paper-note">Uses synthetic document finetuning to implant new beliefs in models.</span>
  </li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2312.07779">Tell, Don't Show: Declarative Facts Influence How LLMs Generalize</a></span> (Meinke and Evans, 2023).
    <span class="paper-note">How training on abstract declarative statements can influence model behavior.</span>
  </li>
  <li><span class="paper-title"><a href="https://arxiv.org/abs/2601.10160">Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</a></span> (Tice et al., 2026)</li>
</ul>

<h2 id="videos">Videos</h2>

<ul class="paper-list">
  <li><span class="paper-title"><a href="https://www.youtube.com/watch?v=eb2oLHblrHU">Podcast interview with Owain Evans</a></span></li>
  <li><span class="paper-title"><a href="https://physics.allen-zhu.com/">Physics of Language Models — video lectures by Zeyuan Allen-Zhu</a></span></li>
</ul>

<h2 id="cite">To cite this primer</h2>

<pre style="background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1rem; font-size: 0.85rem; overflow-x: auto; line-height: 1.5;">@misc{evans2026oocr,
  author = {Evans, Owain},
  title  = {Out-of-Context Reasoning ({OOCR}) in {LLMs}: A Short Primer},
  year   = {2026},
  url    = {https://owainevans.github.io/oocr-primer.html}
}</pre>

</body>
</html>
